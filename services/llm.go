package services

import (
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"os"
	"time"
)

// LLMService - LLM ì„œë¹„ìŠ¤ (Ollama ë˜ëŠ” OpenAI í˜¸í™˜)
type LLMService struct {
	BaseURL    string
	Model      string
	APIKey     string // OpenAI í˜¸í™˜ APIìš©
	HTTPClient *http.Client
}

// OllamaRequest - Ollama API ìš”ì²­
type OllamaRequest struct {
	Model    string          `json:"model"`
	Messages []OllamaMessage `json:"messages"`
	Stream   bool            `json:"stream"`
	Options  *OllamaOptions  `json:"options,omitempty"`
}

// OllamaMessage - Ollama ë©”ì‹œì§€
type OllamaMessage struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

// OllamaOptions - Ollama ì˜µì…˜
type OllamaOptions struct {
	Temperature float64 `json:"temperature,omitempty"`
	MaxTokens   int     `json:"num_predict,omitempty"`
}

// OllamaResponse - Ollama API ì‘ë‹µ
type OllamaResponse struct {
	Model     string        `json:"model"`
	CreatedAt string        `json:"created_at"`
	Message   OllamaMessage `json:"message"`
	Done      bool          `json:"done"`
}

// NewLLMService - LLM ì„œë¹„ìŠ¤ ìƒì„±
func NewLLMService(baseURL, model string) *LLMService {
	return &LLMService{
		BaseURL: baseURL,
		Model:   model,
		HTTPClient: &http.Client{
			Timeout: 60 * time.Second,
		},
	}
}

// NewLLMServiceFromEnv - í™˜ê²½ë³€ìˆ˜ì—ì„œ LLM ì„œë¹„ìŠ¤ ìƒì„±
func NewLLMServiceFromEnv() *LLMService {
	baseURL := os.Getenv("LLM_BASE_URL")
	if baseURL == "" {
		baseURL = "http://localhost:11434" // ê¸°ë³¸ Ollama ì£¼ì†Œ
	}

	model := os.Getenv("LLM_MODEL")
	if model == "" {
		model = "llama3.2" // ê¸°ë³¸ ëª¨ë¸
	}

	apiKey := os.Getenv("LLM_API_KEY") // OpenAI í˜¸í™˜ APIìš©

	log.Printf("ğŸ¤– LLM Service: %s (model: %s)", baseURL, model)

	return &LLMService{
		BaseURL: baseURL,
		Model:   model,
		APIKey:  apiKey,
		HTTPClient: &http.Client{
			Timeout: 60 * time.Second,
		},
	}
}

// callOllama - Ollama API í˜¸ì¶œ
func (ls *LLMService) callOllama(systemPrompt, userPrompt string) (string, error) {
	messages := []OllamaMessage{
		{Role: "system", Content: systemPrompt},
		{Role: "user", Content: userPrompt},
	}

	reqBody := OllamaRequest{
		Model:    ls.Model,
		Messages: messages,
		Stream:   false,
		Options: &OllamaOptions{
			Temperature: 0.7,
			MaxTokens:   200,
		},
	}

	jsonData, err := json.Marshal(reqBody)
	if err != nil {
		return "", fmt.Errorf("JSON ë§ˆìƒ¬ë§ ì‹¤íŒ¨: %w", err)
	}

	url := fmt.Sprintf("%s/api/chat", ls.BaseURL)
	req, err := http.NewRequest("POST", url, bytes.NewBuffer(jsonData))
	if err != nil {
		return "", fmt.Errorf("ìš”ì²­ ìƒì„± ì‹¤íŒ¨: %w", err)
	}

	req.Header.Set("Content-Type", "application/json")
	if ls.APIKey != "" {
		req.Header.Set("Authorization", "Bearer "+ls.APIKey)
	}

	resp, err := ls.HTTPClient.Do(req)
	if err != nil {
		return "", fmt.Errorf("LLM ìš”ì²­ ì‹¤íŒ¨: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return "", fmt.Errorf("LLM ì‘ë‹µ ì˜¤ë¥˜ (%d): %s", resp.StatusCode, string(body))
	}

	var ollamaResp OllamaResponse
	if err := json.NewDecoder(resp.Body).Decode(&ollamaResp); err != nil {
		return "", fmt.Errorf("ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: %w", err)
	}

	return ollamaResp.Message.Content, nil
}

// GenerateCommentary - í•´ì„¤ ìƒì„± (ì™¸ë¶€ì—ì„œ í˜¸ì¶œ ê°€ëŠ¥)
func (ls *LLMService) GenerateCommentary(eventType, context string) (string, error) {
	systemPrompt := `ë‹¹ì‹ ì€ AGV ë¡œë´‡ "ì‚¬ì´ì˜¨"ì˜ ì‹¤ì‹œê°„ eìŠ¤í¬ì¸  í•´ì„¤ìì…ë‹ˆë‹¤.

ğŸ™ï¸ í•´ì„¤ ìŠ¤íƒ€ì¼:
- ì—´ì •ì ì´ê³  í¥ë¶„ëœ í†¤
- ì§§ê³  ì„íŒ©íŠ¸ ìˆëŠ” ë¬¸ì¥ (2-3ë¬¸ì¥)
- ë¦¬ê·¸ì˜¤ë¸Œë ˆì „ë“œ ì‚¬ì´ì˜¨ ìºë¦­í„°ì˜ íŠ¹ì„± ë°˜ì˜
- í•œêµ­ì–´ eìŠ¤í¬ì¸  ì¤‘ê³„ ìŠ¤íƒ€ì¼
- ì´ëª¨ì§€ ì ì ˆíˆ ì‚¬ìš©

âš ï¸ ì£¼ì˜ì‚¬í•­:
- ë°˜ë“œì‹œ 2-3ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ
- ê¸°ìˆ ì ì¸ ìš©ì–´ë³´ë‹¤ ì¬ë¯¸ìˆëŠ” í‘œí˜„ ì‚¬ìš©`

	return ls.callOllama(systemPrompt, context)
}

// Chat - ì¼ë°˜ ì±„íŒ… ì‘ë‹µ
func (ls *LLMService) Chat(userMessage string) (string, error) {
	systemPrompt := `ë‹¹ì‹ ì€ AGV ë¡œë´‡ "ì‚¬ì´ì˜¨"ì…ë‹ˆë‹¤.
ë¦¬ê·¸ì˜¤ë¸Œë ˆì „ë“œì˜ ì‚¬ì´ì˜¨ ìºë¦­í„°ì²˜ëŸ¼ ê°•ì¸í•˜ê³  ë¶ˆêµ´ì˜ ì˜ì§€ë¥¼ ê°€ì§„ ì„±ê²©ìœ¼ë¡œ ëŒ€í™”í•©ë‹ˆë‹¤.
ì§§ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.`

	return ls.callOllama(systemPrompt, userMessage)
}

// IsAvailable - LLM ì„œë¹„ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
func (ls *LLMService) IsAvailable() bool {
	url := fmt.Sprintf("%s/api/tags", ls.BaseURL)
	resp, err := ls.HTTPClient.Get(url)
	if err != nil {
		return false
	}
	defer resp.Body.Close()
	return resp.StatusCode == http.StatusOK
}
